<?xml version="1.0" encoding="UTF-8" standalone="no"?><document><body>. 
Software development or any other research work would require a substantial amount of domain knowledge for successful design and implementation of the solution. Study of existing solution or other similar solution would be of high importance for understanding domains of the problem. Proper understanding of theories and concepts behind solutions to similar problems would allow researchers to identify pitfalls in those solutions and ways to improve and integrate these with proper customizations to solve the problem in hand or to come up with a novel strategy. . 
This document provides a detailed description of the literature review done for SIGMAC project. Furthermore candidate techniques, methodologies and tools identified to support for solving each of the sub problems that needs to be solved for concept map generation from unstructured text is addressed in this document. . 
These sub problems can be interrelated. These are only high level core problems. Subsequent section describing each of these sub problems would address further issues in each of these areas. Identified sub problems for SIGMAC project as follows.. 
Natural Language Processing. 
Concept Extraction. 
Relationship Extraction. 
Visual Representation and Browsing. 
Document preprocessing . 
Document collection could contain different types of documents. Formatting methods used in each type could differ significantly. Therefore each of these documents needs to be converted to an intermediate textual representation. This is done at the Document prepressing stage. Specific details about document preprocessing would be provided in the relevant section.. 
Some of the concept and relationship extraction techniques require natural language processing abilities. The level of language processing differs based on the technique, but higher level processing would require the methods used in the lower level processing. Therefore the section on Natural Language Processing would cover the all information about the NLP techniques used in other sections of this document.. 
Sections on concept extraction and relationship extraction discuss the available techniques and tools studied in the literature review. The section on Visual Representation and Browsing contains discussions about the candidate tools and methods studied during the literature review.. 
The task of Word Sense Disambiguation is to identify the correct sense of the word in context. Words can have different senses. Some words have multiple meanings. This is called as polysemy and sometimes two completely different words are spelled same. For example “Can” can be used as model verb (I can run fast) or as container (I delivered a can of coca cola).  This is called Homonymy. The different between polysemy and Homonymy is always not clear. So the Word sense disambiguation is the problem of identity the correct sense of the word in a given sentence. LEXAS is a good algorithm that supports accuracy for word sense disambiguation.  . 
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological forms, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. They tested it in a common dataset including the noun “interest” used by Bruce and Wiebe who got the 78% of accuracy previously[1].   After LEXAS tested, they could get 87.7% accuracy that is higher than accuracy got by Bruce and Wiebe. . 
For example, let’s take a sentence “The interest rates of fixed deposits are growing rapidly”.  Here the word “interest” is appears as a noun. So LEXAS only consider about the noun meanings of “interest” rather than verb meaning. There might be some morphological form of the selected word. For this example interest might be in forms of “interest”, “interests”. If we consider fall as an example it might be in forms of “fall”, “fallen”, ”fell”, “falls” and “falling”. So LEXAS is doing this convention by converting each word in input sentence into its morphological root using morphological analyzer of WORDNET.  . 
For performing WSD, LEXAS uses training corpus of sentences which are pre tagged with their correct senses[1]. LEXAS builds one exemplar based classifier for each content word w. It operates in two phases. Those are training phase and test phase. In the training phase, LEXAS is given a set S of sentences in the training corpus in which sense-tagged occurrences of w appear. For each training sentence with an occurrence of w, LEXAS extracts the parts of speech (POS) of words surrounding w, the morphological form of w, the words that frequently co-occur with w in the same sentence, and the local collocations containing w. For disambiguating a noun w, the verb which takes the current noun was the object is also identified. This set of values form the features of an example, with one training sentence contributing one training example.. 
In the test phase LEXAS is given unseen sentences. For a new sentence containing w, LEXAS extracts the new set of features value for new sentence including pats of speech of words surrounding w, the morphological forms of w, the frequently core occurring words surrounding w, the local collocations containing w, and the verb that takes w as an  object.  These values forms features for the test example.  Then compare every test example with the every training example and select the closest match one. To find the closest one firstly what they do is to identify the features of the sentence. . 
Let  denotes the conditional probability of sense i of w given word k, where . 
is the number of sentences in which keyword k co-concurs with w.. 
is the number of sentences in which keyword k co-concurs with w where w has sense i.. 
After calculating the conditional probability, LEXAS check whether the keyword k is a feature or not. To select the keyword k as a feature it should satisfy following three conditions.. 
has to be greater than M1 where  M1is predefined value.. 
Word k has to occur at least M2 times with the word w.. 
For the word w, only a M3 number of keywords are selected. This is done by ordering the keywords by the frequency of co-occurrence with the given word w in the sense i.. 
 So the next thing is to find the distance between training and test example. LEXAS uses following definition of distance two symbolic values v1 and v2 of a feature f.. 
C1,i –Number of training examples with value v1 for feature f for sense i  in the training corpus. 
C1-Number of training examples with value v1 for feature f for any sense in training corpus. 
C2,i-Number of training examples with value v1 for feature f for sense i  in the training corpus. 
C2-Number of training examples with value v1 for feature f for any sense in training corpus. 
n-Total number of sense in word w. 
WordNet [1] is a large, well known lexical database for English Language developed by Cognitive Science Laboratory of the Princeton University, United States.  Words of the English language such as Nouns, Verbs, adjectives, adverbs are grouped in to sets called synsets.Currently there are about 117,000 synsets available in WordNet. Each synset express distinct concept. Thosessynsets are linked various semantic relations.WordNet's is developed in order to use it as a tool for computational linguistics and natural language processing. This can be used as a dictionary or a thesaurus. WordNet Interlinks specific senses of words and it labels semantic relationships. Following are those types of sementic relationships.. 
Synonymy is a lexical relation between word forms. It is about similarity of meanings. It is convenient to assume that the relation is symmetric: if x is similar to y, then y is equally similar to x.. 
Definition -two expressions are synonymous in a linguistic context C if the substitution of one for the other in C does not alter the truth value.. 
WordNet contains eight different senses of the highly polysemous noun “case”. 
{carton, case0, box,@ (a box made of cardboard; opens by flaps on the top)}. 
{case1, bag,@ (a portable bag for carrying small objects)}. 
{case2, pillowcase, pillowslip, slip2, bed linen,@ (a removable and washable cover for a pillow)}. 
{bag1, case3, grip, suitcase, traveling bag ,@ (a portable rectangular traveling bag for carrying clothes)}. 
{cabinet, case4, console, cupboard ,@ (a cupboard with doors and shelves)}. 
{case5, container ,@ (a small portable metal container)}. 
{shell, shell plating, case6, casing1, outside surface,@ (the outer covering or housing of something)}. 
{casing, case7, framework,@ (the enclosing frame around a door or window opening)}. 
Antonymyis kind of opposite relation to the synonymy. Extact definition is not there in any literature.The antonym of a word x is sometimes not-x, but not always. As an example consider {rise, ascend} and {fall, decent}. Here {rise, fall} are consider as antonym but {rise, decent} are not considered as antonym.. 
Hyponymy is semantic relation between word meanings.e.g., {maple} is a hyponym of {tree}, and {tree} is a hyponym of {plant}.. 
Meronymya semantic relation—is the part-whole (or HAS A) relation, known to lexical semanticists as meronymy / holonymy.A Concept represented by the synset {x, x′ ,... } is a meronym of a concept represented by the synset { y, y′ ,... } if native speakers of English accept sentences constructed from such frames as A y has an x (as a part) or An x is a part of y. These relations are transitive and asymmetrical.. 
Morphological Relations is a class of lexical relations between word forms. A word can express as varies morphological forms. As a example the verb ‘fall’ can be mentioned in a text as fall, fell, fallen etc. WordNet facilitate all these morphological forms by using morphological relations.. 
Adjectives . 
In WordNet there are two classes of adjectives: descriptive and relational. Descriptive adjectives describe the nouns and typically bipolar. Relational adjectives are assumed to be stylistic variants of modifying nouns.. 
Anaphora resolution is a complicated problem in Natural Language Processing (NLP). There are various definitions in anaphora and one of that is based on the notion of cohesion. That means Anaphora is cohesion which point back to some previous item. The reference is called an anaphor and the entity which it refers is its antecedent. The process of determining antecedent of an anaphor is called Anaphora Resolution. . 
Anaphora resolution can be described as resolving what a pronoun, or a noun phrase refers to. This is also called as co-reference resolution. This can be clearly explained using an example.. 
Java is a programming language. It is a general-purpose, concurrent, class-based, object-oriented language.. 
Here human brain can identify that ‘It’ means ‘Java’ but in computations there should be an algorithm to identify them. In that case relevant words are tagged for resolve the anaphora.. 
It =&gt; Anaphor. 
Java (N)  =&gt; Antecedent. 
If there is a noun phrase (NP) instead of noun whole noun phrase is the antecedent. (Not only noun part). 
Co-reference =&gt; Both ‘Java’ and ‘It’ refer to the same real world entity.. 
So now anaphora resolution can be explained as determining the antecedent of an anaphor. Co-references can be exists as a chain when more than one of preceding entities have the same referent. In co-reference resolution it has to all co-reference chains. There are three major types of anaphora.. 
Pronominal anaphora – Mostly used type of anaphora. This can be resolved by using anaphoric pronoun. (E.g. Above mentioned example). 
Definite noun phrase anaphora – Here antecedent refer by a noun phrase of same or close concept. (E.g. Java is a programming language. The language is a general-purpose, concurrent, class-based, object-oriented language). 
One-anaphora – Anaphoric expression identified using noun phrase ‘one’. (E.g. The small ones went in the side pocket.). 
Also anaphora can be categorized in following way.. 
Intra-sentential anaphora – Both anaphor and antecedent which referenced anaphor are in the same sentence.. 
Inter-sentential anaphora – Referred antecedent which is in a different sentence. Anaphor and antecedent are in two different sentences.. 
Hobbs propose an algorithm[2] that searches parse threes antecedents of a pronoun. . 
Steps in Algorithm. 
1. Begin at the NP immediately dominating the pronoun.. 
2. Go up tree to first NP or S encountered.. 
Call node X, and path to it, p.. 
Search left-to-right below X and to left of p, proposing any NP node which has an NP or S between it and X.. 
3. If X is highest S node in sentence,. 
Search previous trees, in order of recency, left-to-right, breadth-first, proposing NPs encountered.. 
4. Otherwise, from X, go up to first NP or S node encountered,. 
Call this X and path to it p.. 
5. If X is an NP, and p does not pass through an N-bar that X immediately dominates, propose X.. 
6. Search below X, to left of p, left-to-right, breadth-first, proposing NP encountered.. 
7. If X is an S, search below X to right of p, left-to-right, breadth-first, but not going through any NP or S, proposing NP encountered.. 
8. Go to 2.. 
Sample parse tree. 
Sentence: The castle in Camelot remained the residence of the king until 536 when he moved it to London.. 
Figure Figure 3- Sample Parse Tree. 
Here is the parse three after trying to find the reference for “he”. . 
Figure Figure 4- Resultant Parse Tree. 
After executing the algorithm it gives the result “King” as the reference for “he”. 
Resolving anaphora in JavaRAP consist of four steps. They are,. 
1. It uses parser to generate a parse tree using text with sentence delimitation. . 
2. Use the parse tree to extract two lists. They are a noun phrases list with all noun phrases and a list of third person pronouns and reflexive pronouns. . 
3. Then identify the anaphor by considering each item in a subset of the noun phrases (But currently JavaRAP consider only within three sentences from where the anaphor exist). Then antecedent-anaphor pairs are identified by using the anaphor binding algorithm or the syntactic filter, according to the type of anaphor (lexical or a third person pronoun). . 
4. Antecedents which can’t pairs with anaphor by using above step are ranked by their salience weights (Way to measure closeness between antecedent and anaphor) and the antecedent with highest weight is selected as the actual antecedent. Here the antecedent closer to the anaphor is given higher priority.   . 
Reconcile used two famous criteria which used to define set of noun phrases in co-reference relation. They are Message Understanding Conference (MUC) and Automatic Content Extraction (ACE). The name Co-reference Element (CE) stands for any element in co-reference relation. Reconcile consists of both MUC and ACE specification for resolving co-reference. MUC and ACE define noun phrases as nouns, pronouns and noun phrase. Other than that MUC define: Nested named entities (e.g. “Ceylon” in “Bank of Ceylon”), Relative pronouns, Gerunds, Nested nouns (e.g. “union” in “union members”). ACE define,. 
Relative pronouns and gerunds are not co-reference elements (excludes all nested nouns). 
Figure Figure 5 - Co-reference generating algorithmCo-reference elements shouldn’t belong to one of seven semantic classes. (Person, organization, geo-political entity, location, facility, vehicle, and weapon). 
Sentence boundary detection is of high importance higher levels of linguistic processing methods such as POS tagging and chunking. Simplest method of identifying the sentence boundary would be to split a text body in to sentences using the period (“.”).  Although this strategy works in many situations it cannot be taken as an accurate measure due to the ambiguity of the period in a sentence. For example a period may occur in an abbreviation, end of a sentence or in mentioning the names with initials.. 
Different methods for identifying sentence boundaries are suggested by researchers. The approach suggested by David D. Palmer and Marti A.on their paper on Adaptive Sentence Boundary Disambiguation[1] is described below.. 
Stream of text is divided in to tokens. A lexicon containing the frequency of each word occurring in possible part of speech tags are maintained. POS tags are further sub divided in to 18 generic categories. Category frequency of a word is taken as the sum of frequencies of the word in POS tags which was mapped in to the given category. Then the category frequencies are converted to probabilities by dividing the category frequency by the frequency of occurrence of the word. An array containing the category frequencies and two flags to indicating weather word begins with a capital letter, word is followed by a punctuation mark, is created for each word. Then for ‘k’ number of words surrounding an instance of end of sentence, their descriptor arrays are fed to a neural network. Input layer is connected to a hidden layer with j units which are activated by sigmoidal squashing function. The network output a value between 0 and 1. Disambiguation is done based on two threshold values. If the value returned by the network is above the larger threshold value, the input instance is identified as a sentence boundary, if the value is below the lower threshold value; the instance is not a sentence boundary. Value between two thresholds indicate ambiguous input instance.. 
Collocations represent multi word expression which are conventional phrases used in communication. Three different categories were proposed by Frank Smadja[2]. Those are mentioned below.. 
Predicative Relations. 
Rigid Noun Phrases. 
Phrasal Templates. 
Concepts does not necessarily be single words, they might be constructed using multiple words. Therefore identification of these collocations is an important part in concept extraction. The most important category of collocations in concept extraction is Rigid Noun Phrases. These are defined as “uninterrupted sequences of words”. Examples of these are “Database Management Systems”, “Stock Market”, “Object Oriented Programming”. Collocations of this category identification can be done using POS tag signatures. Pure statistical analysis methods for identifying collocations of all three categories were studied in the literature review. Different methods used in collocation identification are also included in the Concept Extraction section of the document.. 
POS tagging is the process of labeling words in a sentence with appropriate POS tags. These POS tags are possible syntactic categories that a word can belong to in its use in a particular sentence. This POS tagged sentences can be used in further level of parsing to generate phrase structure trees for sentences and also in concept identification.. 
Different methods for POS tagging are being used in various POS taggers. Most popular method of POS tagging is developed using the Markov model. Tagging process behaves according to the following two conditions. These are essentially Markov properties that are required to model a process as a Markov process. . 
POS tag of a word does not depend on the previous POS tag of the previous word (Markov Property).. 
The probabilities are time invariant (probability does not change as the sentence is processed).. 
The maximum likelihood of getting a tag A after a particular tag B calculated using relative frequency of different tags followed by particular tag.. 
Set of states in the model corresponds to the set of tags. Probability of getting a particular tag for a given word can directly be computed using the training corpus. That is, the probability of emission of a word from a particular state at a certain position of a word sequence. Let W be the emitted word from state (tag) T. then, . 
Then the most suitable tag sequence for given set of word sequence in a sentence can be calculated as follows.. 
Let W1, n be the word sequence and let T1,n be a possible tag sequence. From the Bayes theorem,. 
From Bayesian statistics we can ignore P(W1,n) in finding the maximum of P(T1,n|W1,n).. 
Then . 
This can be reduced to. 
Assumptions of words being independent are made in above calculations. Therefore the optimal tag sequence for a sentence can be determined as follows. 
Then . 
But this method only can identify the tags for words that are in the training corpus. Different methods for tagging unfound words in the training corpus have also been developed by researchers. Stanford POS tagger[3] is one of the widely used tagger.. 
Concept extraction from documents or individual sentences is a heavily researched area. Several methodologies have been developed by researchers to achieve this task. Each of these methods has their advantages and drawbacks. Some of the techniques depend solely on statistical measures while other techniques utilize semantic information of words from semantic databases to identify concepts. Techniques based on the syntactic structure of the sentences and hybrid techniques which involve syntactic analysis of sentences, use of semantic information and applying statistical information has also been developed. Following section of the document describes the identified methods for concept extraction during the literature review.. 
This technique uses POS tagged sentences as its input and tries to identify candidate concepts that can be used to represent document. The main use of this technique is to identify fixed collocations, but this can be employed to identify concepts as well. Candidate concept extraction is done based on the heuristic that concepts in a sentence should follow particular set of POS tag signatures. When the document is given, first step is to parse the sentences using a POS tagger. Then the POS tagged sentences are inspected to check predefined set of POS tag patterns. Pattern matching is based on greedy approach where largest match is taken in to consideration. If matches are found those words or fixed collocations are taken as candidate concepts. After identifying candidate concepts frequency of occurrence of candidate concepts in the document are taken filter out less frequent concepts and the remaining candidate concepts which have higher frequency is taken to represent the document.. 
Various set of POS tag patterns has been proposed by different researchers. Some of the example patterns suggested are listed below.. 
POS tag patterns suggested by Paul Buitelaar, Thomas Eigner in their research about topic extraction from scientific documents[4] are as follows, other patterns can be found in the VioletaSeretans[5] paper of usage of syntactic patterns for collocation identification.. 
JJ NN – semantic tagging. 
JJ NNS - anaphoric expressions. 
NN NN - ontology adaptation. 
JJ NN NNS - modelling similarity measures. 
NN JJ JJ NN NN - domain specific semantic lexicon construction. 
Set of patterns identified by Justeson and Katz are listed below. These patterns were taken from Foundations of Statistical Natural Language Processing[6] book.. 
A N - linear function. 
N N - regression coefficients. 
A A N - Gaussian random variable. 
A N N - cumulative distribution function. 
N A N - mean squared error. 
N NN - class probability function. 
N P N - degrees of freedom. 
Let’s take the sentence “java applications can run on any operating system” as an example to illustrate the process. POS tagged sentence using Stanford POS tagger would look like the following.. 
java/NN applications/NNS can/MD run/VB on/IN any/DT operating/NN system/NN ./.. 
Concepts mentioned in the sentence are “java applications” which has the tag pattern NN NNS and “Operating System” which has the tag pattern NN NN. First set of patterns mentioned above would only identify the “Operating System” as the concept but the second pattern set would identify both “java applications” and “Operating System” as concepts.. 
There are some drawbacks in this approach. Set of concepts identified can contain some words or collocations which have the same signature but are not concepts described in the document. Accuracy of this improved by the frequency filtering but still it would contain some of the unexpected concepts depending on the source of the text corpora. Furthermore this method does not consider about different senses for words. Similar concepts represented by words would get counted for two different concepts thus resulting in lower frequency of occurrence. This is a common problem for all the methods that are not supported by semantic information about words.. 
In this model terms with highest frequency of occurrence is taken as candidate keywords to represent the document[6]. This model improve its result by filtering out terms which are common in all documents such as the, a, an etc. This list is called the stop word list. In order to identify multi term keywords, technique uses n-gram models. For key words with two terms, co-occurrence of two words is taken as a measure of relationship between words. Set of bigrams with high frequencies are taken as candidate keywords. . 
This technique suffers from the fact that extracted key words might not necessarily represent concept discussed in the document. It can include multi terms such as “he said”, “last year”, in its results. POS tag pattern based filtering method can provide better results than this technique. Concept identification without considering semantic information can also affect the results in cases where synonyms and homonyms of words play a major role in document.. 
Phrase structure trees of sentences in a document are used to identify concepts[7]. These phrase structure trees are generated using syntactic parsing of sentences using language grammar parsers. Sentences are given a tree structure so that noun phrases, verb phrases etc. of sentence can be traversed using tree traversing algorithms or patterns in the tree can be matched using applying pattern matching on the tree itself.. 
First, sentences in the document are parsed using language grammar parse and then noun phrases of sentences are taken as candidate concepts. Then the selected set of concepts is filtered out using frequency filtering where candidate concepts with heights frequency of occurrence are retained to represent document. The idea behind this method is that, concept are likely to be noun phrases in a sentence rather than verb phrases. Here the sub trees rooted under verb phrase nodes could also contain noun phrases. These phrases are also included in finding candidate concepts to represent the given document.. 
Let’s take the sentence “Java is a programming language” as an example. The phrase structure tree generated by the Stanford Parser would look like that of the following figure.. 
Here the noun phrases of the sentence are denoted by NP. Two noun phrases for this sentence are “Java” and “a programming language”. As it can be seen in the second noun phrase unwanted determiner is attached to the noun phrase. It can be removed using the strategy described below.. 
Extension to this method is to use further filtering of candidate concepts using POS tag patterns. This will allow us to filter out unwanted components from the noun phrases.. 
 Semantically blind frequency analysis is another major problem in this method as in other syntax based or frequency based methods. Phrase structure itself can help in identifying difference in meaning therefore considering positioning of noun phrases in the phrase structure is suggested as a possible improvement. But how this can be employed is still under study in the research community.. 
Further processing of phrase structure trees of sentences are done to generate dependency relationships between words. This parsing provide detailed information such as direct object, subject etc. First the sentences in the document are processed to extract dependency relationships. Then the subjects and objects of a particular verb are taken as candidate concepts. Further frequency filtering is used to filter out low frequency concepts.. 
Following shows the dependency relationships identified by RelEx dependency relationship extractor for the sentence “java is a programming language.”. 
_obj(be,language)_subj(be,java)tense(be,present)inflection-TAG(be,.v)pos(be,verb)pos(.,punctuation)_nn(language,programming)inflection-TAG(language,s)pos(language,noun)noun_number(language,singular)inflection-TAG(programming,n-u)pos(programming,noun)noun_number(programming,uncountable)inflection-TAG(java,.n)pos(java,noun)noun_number(java,uncountable)pos(a, det). 
This technique would extract “language” and “java” as concepts from the sentence.Details of the dependency relationships are discussed under the section describing the RelEx dependency relationship extractor.. 
One drawback of this method is that, the concept identification is dependent on the dependency relationship generation which is still not fully capable of identifying multi term concepts. Therefore further level is parsing is used to identify multi term concepts instead of just extracting subjects and objects in dependency relationships. Accurate selections of dependency relationships are required for concept identification. Inappropriate selection of relationships to be used would result in exclusion of important concepts or inclusion of unwanted concepts to represent document.. 
This method requires access to a database with semantic information of words. This is one of the methods utilized by Boris Gelfand in Automatic Concept Extraction from plain text[8]. Synonyms and homonyms are taken in to account when identifying concepts. Synonym and homonym identification is done using the contextual information in occurrence of words. That is, the set of words in the sentence where candidate word occurs are taken in to account to identify synonyms and homonyms. Concept relationships are also derived from sematic source using concept hierarchies available in the semantic database. Frequency of occurrence is used to identify key concepts to represent the document. This method gives more accurate set of concepts to represent document.. 
The accuracy of this technique is heavily dependent on the semantic data source. Comprehensive sematic data sources of all possible words are not feasible or they are impossible. Another problem with this technique is that the words can have different senses depending on the domain and the position in which the word appears in the sentence. Covering all this base would be a complex task. Therefore this can lead to abandoning concepts which are not present in the semantic database. A domain dependent semantic data source can be helpful in concept extraction from documents of a particular domain. Expertise of the domain knowledge and linguistic knowledge is required to create such a database. Maintenance of such information need to been done in order to support the evolution of phrase used in the selected domain.. 
Most of the documents are not in plane text format. Therefore those documents contain some formatting information about the content of the document. This formatting information such as font size, bold text, can be used to support concept identification process. These support information can be extracted in the preprocessing stage of documents prior to linguistic analysis of sentences. Topics in each segment of the document can be identified using font-sizes. The heuristic behind this process is that the topic of a segment would have larger font-size. Identified topical segment does not necessarily represent concepts. Therefore this information would only be used for supporting concept extraction process. Accurate linguistic processing to improve the information in topical segments is also not possible because, most of the topical segments dose not form grammatically correct sentences.. 
The major difficult of this method arises due to the need of document parsers for each document type that need to be analyzed. Each document type would need to be handled independently since there is no universal parsing mechanism to identify the formatting information in each of the document types. . 
In this method k-gram define as an ordered sequence of k words occurring one after the other in text.  For example “Sanath Jayasuriya is a good cricketer” has three grams: “Sanath Jayasuriya is”, “Jayasuriya is a”, “is a good”, “a good cricketer” and has four grams: “Sanath Jayasuriya is a”, “Jayasuriya is a good”, “is a good cricketer”. In this is method it is assume that all the Wikipedia titles as concepts. . 
 For a given dataset C to find the concepts what the preprocessing step does is to extract all the k-grams with their frequencies. Since very few concepts have more than four words this algorithm considers only the k ⩽4 grams[3]. . 
If a given k-gram a1a2 … ak (k &gt; 2) is a concept, then it is not true that both of the following k–1-grams are concepts: a1a2 … ak-1 and a2a3 … ak. If a 2-gram a1a2 is a concept, then at least one of a1 and a2 are concepts.. 
Table Table 1 -Claim JustificationThis claim has been justified from analyzing Wikipedia articles[3].. 
From analyzing the justification table we can see that the claim is not 100% accurate. So there is a possibility to miss some concepts, but the possibility is too small. At the first extract the k-grams which can be considered as concepts. Rest of the k-grams can be discarded. To find the k-grams which are to be considered as concepts, this method use pre-defined set of k-grams that are concepts.  From the above claim it can be discard either k-gram or some k-1 gram that the k gram contains. In order to extract concepts from above filtered words, following indicators are used. . 
First Indicator . 
In a concept frequency of occurrence is high. Frequency of occurrence of concept a is also called as support Sa.. 
Second Indicator. 
What we select as the concept should be better than the sub and super concepts. In this algorithm it has been defined three threshold values to satisfy second indicator.  . 
If a=t1t2t3…tk, then b=t1t2….tk-1 is the prefix k-1 gram and c=t2t3…tk is the suffix k-1 gram. Following confidence level should be exceed the threshold values for satisfy the second indicator. . 
Pre-conf : . 
Post-conf : . 
Third Indicator. 
This indicates that only portion of sentences that convey a single meaning or idea. . 
Algorithm 1 takes k-grams with annotated frequencies and proceeds to extract concepts in bottom up manner.  Algorithm 1 calls to algorithm 2 to check whether the given k-gram satisfies the above indicators 1 and 2. . 
To satisfy the second indicator, it should be exceeded some threshold values. Those are listed below for each k-gram[3].. 
Parameter. 
k=1. 
k=2. 
k=3. 
k=4. 
Support-threshold. 
100. 
35. 
60. 
60. 
Min-conf threshold. 
-. 
0.035. 
0.1. 
0.15. 
Rel-conf threshold. 
-. 
-. 
0.2. 
0.25. 
Pre-conf threshold. 
-. 
0.1. 
0.15. 
0.2. 
Huge amount of usable electronic data is in the form of unstructured text which cannot be easily understood by machines. Relationship extraction is a task that recodes those unstructured information found on those text, into well-structured format. In our project the domain is computer science and related technologies. Adapting conventional relation extracting frameworks requires significant effort. Here we analyze the current approaches to achieve those goals.. 
Here we are going to discuss about the relationship among the concepts which extracted from technical documents. In our project requirements the relationships should not only be hierarchical it can and it will not be a tree, more like a linked map of nodes. This map will not be radial hierarchy or tree structure as in mind maps. Nodes are the concepts and the edges linking those nodes will be the relationships. These relationships can be classified and quantified. Edges should represent how close those concepts (proximity). . 
Supervised. 
This includes rule engineering and supervised machine learning techniques. For a new domain, system needs to be built from a starch.. 
Bootstrapping . 
Using initial seed data system can be build. This Approach is less costly compared to the supervised method in terms of effort and less complex.. 
Generic Approaches. 
It can be easily adapted to a new domain using generic approaches because of the. 
Parameters do not need to be modified for new domains or tasks.. 
From above supervised and bootstrapping approaches are mainly focused on instantiation of ontology while generic approaches address problem of learning relation schemas from data. . 
Relationship Extraction can be divided in to two main tasks. First one is relationship identification (also known as relation mining) and other one is relationship characterization (also known as relation discovery).. 
Goal of the information extraction framework is extracting structured information form text written in natural language. Information extraction framework consists of solutions for different sub problems. Core tasks that can be identified are named entity recognition (NER), relationship extraction (RE) and event extraction (EE). . 
Here the goal is to identify mentions of relations from the text. Following is the definition for relationship mention.[2]. 
“A relation mention is a predicate ranging over two arguments, where an argument represents concepts, objects or people in the real world and the relation predicate describes the type of stative association or interaction that holds between the things represented by the arguments “. 
Occurrences of textual references to a relation are referred to here as relation mentions. Likewise, instances of textual references to an entity are referred to as entity mentions. Finally, labels used to describe the class of relations and entities are referred to as relation types and entity types.. 
Figure 2 represent pipeline of two main sub tasks of relation identification and relation Characterization. Input is documents containing unstructured text written in natural language. These documents are fed into a relation identification system which identifies pairs of relation forming entity (or concept) mentions. Then the relation characterization system annotates the entity mention pairs with a label which is describing the type of the relationship.. 
Traditional approaches to relation extraction are based on either rule engineering or supervised machine learning. Rule engineering needs an expert in targeted domain in order to develop extraction grammar. A rule should be there to extract each relation on the text. In supervised learning previously tagged or annotated corpus need to fed to the system in order to train the new system. Both concepts or entities and relations mention should have been marked by a human annotator. Most of the time supervised machine learning systems extracts features of entity mention together with surrounding context of a sentence.. 
Bootstrapping approaches can be further categories in to three types. In the first type it is required to have pre-tagged training data like in supervised machine approaches, but uses relatively small amount. Then the widely coverage system will do the relation extraction though the iterative process of learning and automatic annotating new data. In the second type it is required to have small amount of entity pairs for each type of specific relation.then the system will bootstrapped in a iterative process.. 
Identify the sentence segments where the training entities occur together and using those to identify patterns for relationship identification task.. 
Using patterns identified by step 1 to identify new pairs which the intended relation exists.. 
Third type requires set of documents tagged as relevant or non-relevant to a particular relation extraction task. The difference among relevant and non-relevant is used to assign a weight that balances pattern frequency in relevant documents versus frequency across relevant and non-relevant documents.. 
Generic approaches are different from above two approaches because no need to have annotated data or parameter tuning for relation extraction in new domain. Generic approaches can be developed with reference to one domain and transfer other domain without losing the accuracy significantly. They can be used for learning the structure of ontologies and learning how to instantiate them. Conrad and Utt (1994) describe an approach to identifying associated pairs of named entities from a large corpus using statistical measures of co-occurrence. Hasegawa et al. (2004) describe an approach to characterizing co-occurring named entity pairs by relation type.. 
Ontologies are the formal representations of major terms and concepts of domains with their relations. These Ontologies are heavily used in bio medical sub domains with natural language processing (bio-NLP). This should have been an easy task if we have complete open centralized ontology for computer science. For Bio-Medical domain currently there are several ontologies developed for subdomains. (Ex: http://obofoundry.org/).. 
Many researchers are mostly interests in relations such as hypernymy (or is-a), meronymy (or part-whole), causality (relationship between the cause and the effect) etc. Other that those common relations there researches have studied identifying are many relationships such as located in, book-authored-by, employee-organization, film-director etc.. 
Once a relationship was identified researches investigate the linguistic patterns that can express that relationship. Then the specific knowledge patterns are used to identify those relationships. . 
Traditionally they used machine readable dictionaries (MRD) to acquire semantic relations. And Domain specific semantic relationships also can be acquired by using those MRDs. This was convenient when there were small size digital textual data available. But now with the availability of very large textual repositories, data mining techniques and algorithms are used for this task. . 
Also the internet can be used for finding patterns for specific domain. Since here in our project we are feeding hand crafted corpus which is reliable but sometimes it will be inadequate to find relationship data of two concepts. So as shape up the concept map we need to extra relationship data which can be obtained by crawling related web pages. By using some filtering strategies we can refine those identified relationships.. 
In the pattern base approach the terminology is used for those patterns is lexico –syntactic patterns. Those lexico syntactic patterns are expressed with combinations of Part of Speech (PoS) tags. For an example a typical hypernymy (or is-a) pattern is expressed as. 
&lt;NP0&gt; is-a &lt;NP1&gt; which ….. 
Here the &lt;NP0&gt; and &lt;NP1&gt; are PoS tags for noun phrases of a sentence. . 
There are lot of research are going on hypernymy extraction. All those approaches can be categorized in to following types.. 
Analyzing the syntagmatic relations in a sentence. 
Analyzing the paradigmatic relations in a sentence. 
Document clustering. 
These types of methods usually employ set of PoS tag patterns to identify Hypernymy extraction. Popular pattern set is Hearst pattern set &lt; Hearst, M.: Automatic acquisition of hyponyms from large text corpora. In:Proc. of COLING, Nantes, France (1992) &gt;. 
– NPhyper such as {{NPhypo,}* (and|or)}NPhypo. 
– suchNPhyper as{NPhypo,}*{(and|or)}NPhypo. 
– NPhypo{,NPhypo}*{,}or  otherNPhyper. 
– NPhypo{,NPhypo}*{,}and otherNPhyper. 
– NPhyper{,}including {NPhypo,}*{and|or}NPhypo. 
– NPhyper{,}especially{NPhypo,}*{and|or}NPhypo. 
NPhyperand NPhypoare PoS variables which applied to arbitrary texts.. 
Stanford parser[9] is a Java based typed dependency parser. Phrase structure tree can also be extracted at an intermediate step of the parsing. Development of phrase structure is done based on probabilistic context free grammar. These are necessarily context free grammar rules which have associated probabilities. Dependencies identified by the parser are triplets which consist of the name of the relation, governor and the dependent. A total of 53 typed dependencies are identified by the parser. Five different representations of typed dependencies are supported by the parser which is intended to give different level of information and different access methods to efficiently manipulate the output of the parser.. 
Following figure shows the phrase structure tree generated by the Stanford parser for the sentence “Java knowledge is helpful in Android application development.”. 
RelEx is a relation extracting tool which was initially developed by mainly targeting bioinformatics field. Getting details about some biological aspect was difficult, because large scaled biomedical publications need to refer for that. Also they need a way to get relations between some interrelated biological aspects e.g. physical or regulatory interactions between genes and proteins. In case of both above reasons they need a structured way to manage their free text resources. RelEx is the methodology used for that by extracting relationship from free text. But now RelEx is a part of OpenCog’s main project. It is creating an open source Artificial General Intelligence framework (A thinking machine).. 
There are several approaches exist for extracting relationships. Detecting co-occurrence of concepts within the sentence or whole document is one of them. This described, any set of concepts repeatedly occurred together then there should be any relationship between them. But the type and the direction of relation cannot be determined using this approach. Pattern-based extraction is another approach for relationship extraction. Here the extractions are extracted using pre-defined patterns. But this achieves lower recall. But RelEx used dependency parse tree to extract relationships. Dependency parse tree is a rooted tree that represents the syntactic structure of words in a sentence. It provides a structure for the sentences by labeling edges with dependency types (subject, object, indirect object auxiliary, modiﬁer and etc.).. 
The work-flow of RelEx can be subdivided into three parts [1]. They are,. 
Pre-Processing. 
Relation Extraction. 
Relation Filtering. 
Figure Figure 10- Feature tagging typesIn the pre-processing part of RelEx, first the words are tagging using feature tagging methodology (Figure xxx). Here tag the words with many possible properties (OpenNLP). Then identify the noun-phrase chunks by recognizing the chunks that consist of noun phrases. POS tagged sentences are submitted to lexicalized parser (Link Grammar parser [3]) to generate the dependency parse tree. This generated parse tree is again simplified by using the identified noun-phrase chunks by grouping the words into noun phrase chunks.[2] The tree generated by using dependency parse tree and noun phrase chunks is called chunk dependency tree.. 
Relex also includes some more advanced natural language processing methodologies. For identifying the co-relations of nouns they integrated the Hobbs anaphora (pronoun) resolution algorithm (Explained above).. 
RelEx use GATE for entity detection as an option. GATE used an information extracting modules called ANNIE [4]. ANNIE consists of several steps in extracting information.. 
Unicode Tokeniser - Splits the text into very simple tokens such as numbers, words, symbols, punctuations and space token.. 
Gazetteer Lookup - A gazetteer contains an entity list such as countries, cities, organizations etc. They are used to identify occurrences of these entities in text.. 
Sentence Splitter – This is the part which segmented the text into sentences. This module is required before POS tagging the sentences. This used a gazetteer list of abbreviations to differentiate the full stops from dots between abbreviations. Also each sentences can annotated with ‘Split’ and ‘Kind’ (Splitting method). There are two type of splitting.. 
Internal – Any combination of exclamation and question mark or full stop.. 
External – Newline.. 
Semantic Tagger – ANNIE used JAPE language [5] for producing annotated entities.. 
OrthoMatcher – Get the identity relations between entities extracted by semantic tagger. So this identifies the correlations between entities.. 
Those are the things which are done in preprocessing phase of RelEx relation extracting process.. 
Figure Figure 11 - RelEx work-flow. 
Relations between concepts are extracting using the path between them. There are three rules are applied on chunk dependency tree to extract the relationships. They are,. 
Effector-relation-effectee. 
Starting point of the parse tree is called as effector and end point is called as effectee. First step of this rule is to identifying subject dependencies (nsubj or nsubjpass) and split the tree as those subject dependencies becoming the root of each sub trees. Then each tree has only one subjected dependency. Any incoming edge labeled as subject-dependency then those are potential start-points. Then construct the path towards other entities (potential end-points). If there is no any node with subject dependency all the noun-phrase chunks are named as potential start-points and end-points. The path connecting start and end points are extracted from the chunk dependency tree.. 
 But in this process there may be some false path also. They are filtering by checking whether verbs and modifying terms usually occur before the object they refer to. Another filtering approach is, if there are two nodes tagged as verbs between last common ancestor and end-node. In this case first verb node is removed from the path.. 
Relation-of-effectee-by-effector . 
Extracts the longest paths through the tree that contain only noun phrase chunks as nodes and dependencies of the types ‘of’, ‘by’, ‘to’, ‘on’, ‘for’, ‘in’, ‘through’, ‘with’.. 
Relation-between-effector-and-effectee. 
Figure Figure 12- Application of rule type 2 and 3IF two noun phrase chunks connected by ‘between’ dependency type provided that the successor in the tree or has a dependent noun phrase chunk that connected via an ‘and’ dependency. Those kinds of paths are extracted from the dependency tree.. 
Negation check – If the candidate relation or it’s child node consist of negation word (no, not, nor, neither, without, lack, fail(s,ed), unable(s), abrogate(s,d), absen(ce,t)). They are further analysed.. 
Effector-effectee detection – In a small sentence first appeared entity considered as the effector. But in some cases effector and effectee are switched. Below mentioned the terms indicating switched roles.. 
By using above mentioned methodologies many types of information can be extracted using RelEx relationship extractor. Some of them are mentioned below [2].. 
Word properties - Feature tagged properties (tense, number, part-of-speech tagging, etc.). 
Binary relations - Dependency relations (subject, object, possessive relations, etc.). 
Query variables - who, what, where, when, how, which, why.. 
Comparison variables – Comparative indicators ( "more, "as many as", etc). 
RelEx generate an output with different type of relation types. The relationship between two concepts indicates by the first part of output and those two concepts are mentioned with in bracket in an order. As mentioned in above example there are several types of relation types can extracted using RelEx relation extractor. Currently it has ability to extract nearly 56 types of relations. The below mentioned sentence is processed through RelEx and output is shown below.. 
 Input sentence: RelEx is a relation extractor.======Dependency relations:_obj(be, extractor) // Direct Object_subj(be, RelEx)  //Subject of verb ‘be’tense(be, present)  //Tense of the sentenceinflection-TAG(be, .v) pos(be, verb)  //Identify verb pospos(., punctuation)  //Identify punctuation pos_nn(extractor, relation) //Noun modifierinflection-TAG(extractor, .n)pos(extractor, noun) //Identify noun posnoun_number(extractor, singular)  //noun number (singular or uncountable) inflection-TAG(relation, .n-u)pos(relation, noun)  //Identify noun posnoun_number(relation, uncountable)definite-FLAG(RelEx, T)pos(RelEx, noun)  //Identify noun posnoun_number(RelEx, singular)pos(a, det)======. 
 . 
x. 
[1]. 
Hian Beng Lee Hwee Tou Ng, "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach," Defence Science Organisation, 20 Science Park Drive, 1996.. 
[2]. 
Jerry R. Hobbs,. Lingua, 1978, pp. 311-338.. 
[3]. 
Anand Rajaraman,Hector GarciaMolina Aditya Parameswaran, "Towards The Web Of Concepts: Extracting Concepts from Large Datasets," Stanford University, Singapore, 2010.. 
x. 
[1] D. D. Palmer and M. A. Hearst, “Adaptive sentence boundary disambiguation,” 1994, p. 78.. 
[2] F. Smadja, “Xtract: An overview,” Computers and the Humanities, vol. 26, no. 5, pp. 399–413, Dec. 1992.. 
[3] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer, “Feature-rich part-of-speech tagging with a cyclic dependency network,” in Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, Stroudsburg, PA, USA, 2003, pp. 173–180.. 
[4] P. Buitelaar and T. Eigner, Topic Extraction from Scientific Literature for Competency Management. .. 
[5] V. Seretan, “Induction of syntactic collocation patterns from generic syntactic relations,” in Proceedings of the 19th international joint conference on Artificial intelligence, San Francisco, CA, USA, 2005, pp. 1698–1699.. 
[6] C. D. Manning and H. Schuetze, Foundations of Statistical Natural Language Processing, 1st ed. The MIT Press, 1999.. 
[7] D. Gildea and D. Jurafsky, “Automatic labeling of semantic roles,” Comput. Linguist., vol. 28, no. 3, pp. 245–288, Sep. 2002.. 
[8] B. Gelfand, M. Wulfekuler, and W. F. Punch, “Automated concept extraction from plain text,” in AAAI 1998 Workshop on Text Categorization, 1998, pp. 13–17.. 
[9] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” in Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, Stroudsburg, PA, USA, 2003, pp. 423–430.</body><titles><title scale="5">Introduction</title><title scale="5">Natural Language Processing</title><title scale="5">Concept Extraction</title><title scale="5">Relationship Extraction </title><title scale="5">Stanford parser</title><title scale="5">Reference </title><title scale="4">Word Sense Disambiguation (WSD)</title><title scale="4">WordNet</title><title scale="4">Anaphora resolution</title><title scale="4">Sentence Boundary Detection</title><title scale="4">Collocation Identification</title><title scale="4">Parts of Speech Tagging</title><title scale="4">Concept Identification using POS tag signatures</title><title scale="4">Frequency based Key word extraction</title><title scale="4">Dependency Relationship based concept extraction</title><title scale="4">Semantic data source based concept extraction</title><title scale="4">Usage of formatting information to support concept identification</title><title scale="4">K-gram based method</title><title scale="4">Common Relationship Extraction Approaches</title><title scale="4">Ontologies</title><title scale="4">Pattern-based approach to relation extraction</title><title scale="4">Relationship Extraction Frameworks</title><title scale="3">LEXAS (Lexical Ambiguity Resolving System)</title><title scale="3">2.3.1. Hobbs’s “Naive” Algorithm</title><title scale="3">2.3.2 Anaphora Resolution Frameworks</title><title scale="3">Usage of Phrase structure for concept extraction</title><title scale="3">Claim 1</title><title scale="3">K-gram Algorithms</title><title scale="3">The Information Extraction Framework</title><title scale="3">Relation Identification and Characterization</title><title scale="3">Relation Extraction and Adaptation</title><title scale="3">Relations of Interest</title><title scale="3">Pattens</title><title scale="3">Discovery and Pattern Expression</title><title scale="3">Hypernymy Extraction</title><title scale="3">RelEx - Relation extractor</title><title scale="2">Algorithm</title><title scale="2">2.3.3.1 JavaRAP</title><title scale="2">Reconcile</title><title scale="2">Supervised Approaches</title><title scale="2">Bootstrapping Approaches</title><title scale="2">Generic Approaches</title><title scale="2">Analyzing the syntagmatic relations in a sentence</title><title scale="2">Pre-Processing</title><title scale="2">Relation Extraction in RelEx</title><title scale="2">Relation filtering and post-processing steps</title><title scale="1">Synonymy</title><title scale="1">Antonymy</title><title scale="1">Hyponymy </title><title scale="1">Meronymy</title><title scale="1">Morphological Relations </title><title scale="1">Adjectives </title><title scale="1">Java is a programming language. It is a general-purpose, concurrent, class-based, object-oriented language.</title><title scale="1">It =&gt; Anaphor</title><title scale="1">Java (N)  =&gt; Antecedent</title><title scale="1">Co-reference =&gt; Both ‘Java’ and ‘It’ refer to the same real world entity.</title><title scale="1">Pronominal anaphora</title><title scale="1">Definite noun phrase anaphora</title><title scale="1">Java is a programming language. The language is a general-purpose, concurrent, class-based, object-oriented language</title><title scale="1">One-anaphora</title><title scale="1">Intra-sentential anaphora –</title><title scale="1">Inter-sentential anaphora – </title><title scale="1">Steps in Algorithm</title><title scale="1">Sample parse tree</title><title scale="1">Sentence</title><title scale="1">JJ NN – semantic tagging</title><title scale="1">JJ NNS - anaphoric expressions</title><title scale="1">NN NN - ontology adaptation</title><title scale="1">JJ NN NNS - modelling similarity measures</title><title scale="1">NN JJ JJ NN NN - domain specific semantic lexicon construction</title><title scale="1">A N - linear function</title><title scale="1">N N - regression coefficients</title><title scale="1">A A N - Gaussian random variable</title><title scale="1">A N N - cumulative distribution function</title><title scale="1">N A N - mean squared error</title><title scale="1">N NN - class probability function</title><title scale="1">N P N - degrees of freedom</title><title scale="1">java/NN applications/NNS can/MD run/VB on/IN any/DT operating/NN system/NN ./.</title><title scale="1">First Indicator </title><title scale="1">Second Indicator</title><title scale="1">Third Indicator</title><title scale="1">Effector-relation-effectee</title><title scale="1">Relation-of-effectee-by-effector </title><title scale="1">Relation-between-effector-and-effectee</title><title scale="1">Negation check</title><title scale="1">Effector-effectee detection – </title><title scale="1">RelEx is a relation extractor.</title><title scale="1">Dependency relations:</title></titles></document>